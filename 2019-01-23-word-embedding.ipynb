{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "maxlen = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000,), (25000,), (25000,), (25000,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turns lists of integers into a 2D integer tensor of shape\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 20), (25000, 20))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 8)             80000     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 161       \n",
      "=================================================================\n",
      "Total params: 80,161\n",
      "Trainable params: 80,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 3s 172us/step - loss: 0.6759 - acc: 0.6050 - val_loss: 0.6398 - val_acc: 0.6814\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.5657 - acc: 0.7427 - val_loss: 0.5467 - val_acc: 0.7206\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.4752 - acc: 0.7808 - val_loss: 0.5113 - val_acc: 0.7384\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.4263 - acc: 0.8077 - val_loss: 0.5008 - val_acc: 0.7452\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3930 - acc: 0.8258 - val_loss: 0.4981 - val_acc: 0.7538\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3668 - acc: 0.8395 - val_loss: 0.5014 - val_acc: 0.7530\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 3s 140us/step - loss: 0.3435 - acc: 0.8533 - val_loss: 0.5052 - val_acc: 0.7520\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 3s 138us/step - loss: 0.3223 - acc: 0.8657 - val_loss: 0.5132 - val_acc: 0.7486\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 3s 139us/step - loss: 0.3022 - acc: 0.8766 - val_loss: 0.5213 - val_acc: 0.7490\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 3s 142us/step - loss: 0.2839 - acc: 0.8860 - val_loss: 0.5303 - val_acc: 0.7466\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# why 8 dimensional\n",
    "model.add(Embedding(10000,8, input_length=maxlen))\n",
    "# has to be flatten before connecting to dense layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# We add the classifier on top\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/i846240/projects/_deep_learning/deep-learning-with-python-notebooks'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='/Users/i846240/projects/_deep_learning/fastai/courses/dl1/data/aclimdb/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README      imdb.vocab  imdbEr.txt  \u001b[34mtest\u001b[m\u001b[m/       \u001b[34mtrain\u001b[m\u001b[m/\r\n"
     ]
    }
   ],
   "source": [
    "ls {data_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "train_dir = os.path.join(data_path,'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeledBow.feat  \u001b[34mpos\u001b[m\u001b[m/             unsupBow.feat    urls_pos.txt\r\n",
      "\u001b[34mneg\u001b[m\u001b[m/             \u001b[34munsup\u001b[m\u001b[m/           urls_neg.txt     urls_unsup.txt\r\n"
     ]
    }
   ],
   "source": [
    "ls {train_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dir_name):\n",
    "    \"\"\"\n",
    "        load data from directory to list\n",
    "        expects pos and neg subdirectories in dir_name\n",
    "        returns texts, labels as lists\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    texts = []\n",
    "    \n",
    "    for label_type in ['neg', 'pos']:\n",
    "        dir_name = os.path.join(train_dir,label_type)\n",
    "        for fname in os.listdir(dir_name):\n",
    "            if fname[-4:]=='.txt':\n",
    "                f = open(os.path.join(dir_name,fname))\n",
    "                texts.append(f.read())\n",
    "                f.close()\n",
    "                if label_type == 'neg':\n",
    "                    labels.append(0)\n",
    "                else:\n",
    "                    labels.append(1)\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, labels = load_data(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Working with one of the best Shakespeare sources, this film manages to be creditable to it's source, whilst still appealing to a wider audience.<br /><br />Branagh steals the film from under Fishburne's nose, and there's a talented cast on good form.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 100 # We will cut reviews after 100 words\n",
    "training_samples = 200\n",
    "validation_samples = 10000\n",
    "max_words = 10000 # we will only consider the top 10,000 words in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### https://faroit.github.io/keras-docs/1.2.2/preprocessing/text/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get word index from tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 106398 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index['man']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 100), (25000,))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "labels = np.asarray(labels)\n",
    "data.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the GloVe word embeddings\n",
    "https://nlp.stanford.edu/projects/glove/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dir='/Users/i846240/projects/_deep_learning/data/glove.6B/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove.6B.100d.txt  glove.6B.200d.txt  glove.6B.300d.txt  glove.6B.50d.txt\r\n"
     ]
    }
   ],
   "source": [
    "ls {glove_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigarms 0.18917 -0.3181 -0.43749 0.61209 -0.031357 0.1568 0.30505 -0.39915 0.30101 -0.17353 -0.032278 -0.29338 -0.16598 -0.15038 -0.29364 -0.062909 -0.32706 0.13117 0.010177 0.078467 -0.32796 0.008198 -0.34072 0.1286 -0.27844 0.41588 -0.045799 0.45221 -0.065252 -0.055483 0.50665 -0.067356 -0.29428 -0.12657 -0.024669 -0.37284 0.23507 -0.48212 0.041255 -0.081911 0.59964 0.2047 -0.23401 -0.34091 -0.13306 0.36 -0.00697 0.6013 0.50416 0.099127 -0.031218 0.40532 -0.16901 -0.4664 0.14094 0.74539 0.15076 0.18017 -0.51756 -0.59651 -0.20991 -0.5506 -0.023214 -0.4166 -0.33079 -0.048234 -0.16065 0.36674 0.0072976 0.051298 -0.38274 0.57222 -0.34032 0.16816 -0.40298 -0.12421 0.16196 -0.15993 0.43126 -0.4145 -0.48541 -0.28735 0.34193 -0.029869 0.70822 0.01381 -0.034878 -0.26084 0.20595 0.64408 -0.056202 -0.1277 0.28507 -0.16824 -0.018927 -0.24652 -0.10403 0.32856 -0.43073 -0.10308\r\n",
      "katuna -0.21887 -0.30785 -0.28557 0.36186 -0.20892 -0.011439 -0.39597 0.27104 0.51042 -0.29147 -0.18927 0.167 -0.043189 -0.0076813 0.079556 0.28642 -0.12039 -0.79228 0.403 -0.37501 0.28638 0.43372 -0.23248 -0.28213 -0.17521 0.011767 -0.0056339 -0.056547 0.20587 -0.11068 -0.075307 -0.11201 0.81534 0.056497 0.5012 -0.44869 -0.28734 -0.67028 0.086008 -0.0032898 0.25806 0.67041 -0.18061 0.21005 -0.22464 0.28244 -0.12104 0.36402 0.10356 0.29394 0.19594 0.31164 -0.24877 -0.20947 -0.27186 0.66153 -0.1452 0.10883 -0.28422 -0.39554 -0.11977 -0.12497 0.15946 -0.36298 -0.40915 0.086626 -0.31137 -0.39377 -0.48603 0.14716 0.39346 0.0041725 0.2165 0.33835 -0.16381 -0.069797 0.37364 0.29598 0.35004 0.29113 0.012292 0.18861 0.45024 -0.26104 0.40511 0.47224 0.33026 0.23671 -0.17786 0.1739 -0.17542 0.41623 -0.04723 0.23439 0.22071 -0.079093 0.49752 0.4401 -0.25509 0.44544\r\n",
      "aqm 0.37372 0.21216 -0.61747 0.48158 -0.022374 -0.21698 -0.14605 0.17147 -0.29506 -0.49333 0.51952 0.18384 -0.018236 -0.026255 -0.15424 0.55934 -0.032471 -0.18166 0.45563 -0.30715 -0.58712 0.15795 -0.1111 -0.28791 -0.82979 0.20936 -0.37071 0.30697 -0.11828 0.26039 0.24629 0.31701 -0.29463 -0.039123 -0.044956 -0.19887 -0.31857 -0.065253 -0.48279 0.050349 0.019571 0.049414 -0.16211 0.13625 -0.34824 0.25767 -0.55942 0.21233 -0.018795 0.72442 0.33323 -0.25728 -0.27043 -0.13375 0.76019 1.0646 -0.11868 0.32608 -0.24438 -0.72694 0.091176 -0.40616 0.28378 0.085243 -0.67119 0.23653 -0.31338 -0.56921 -0.022608 -0.034588 -0.016904 0.2896 -0.01704 0.13038 -0.22882 -0.53274 0.18616 0.43042 0.65397 -0.51894 -0.095175 -0.41508 -0.23548 -0.61867 0.8877 0.5468 -0.36682 0.076571 0.18696 0.01932 0.46644 0.62652 0.041762 0.1549 0.20668 0.0035888 0.056255 0.035766 -0.34405 -0.32486\r\n",
      "1.3775 0.45771 0.1271 -0.23676 0.058859 0.17576 0.22606 -0.13308 -0.19668 0.016876 0.19004 0.055791 0.34109 -0.31568 -0.0941 0.32461 -0.13585 -0.13675 -0.42524 0.040012 0.17911 -0.062705 0.032896 -0.54879 -0.37689 -0.0025686 -0.036752 -0.1297 0.50618 0.21003 -0.40258 0.45895 -0.1308 0.31515 -0.031106 0.03072 -0.18423 -0.54498 -0.59592 -0.36091 0.1945 0.39025 0.10958 -0.2726 0.48004 -0.27752 -0.11313 -0.13584 0.42531 -0.24104 0.17879 0.4797 -0.14747 -0.36208 -0.41269 0.16858 0.28535 -0.38502 -0.050096 -0.096347 -0.6829 0.12246 0.31126 -0.10906 -0.079203 -0.85002 -0.028526 -0.38133 -0.31596 -0.62216 -0.43796 -0.12496 0.050087 -0.22498 0.38359 -0.028541 0.053482 0.14193 -0.16055 0.57861 0.47895 -0.016293 0.15554 0.36762 -0.45868 0.22228 -0.065948 0.13141 0.30238 -0.29245 0.28341 -0.63989 0.10335 0.096162 -0.10495 0.1979 -0.058495 0.25698 0.71917 -0.13331 -0.1474\r\n",
      "corythosaurus 0.083099 -0.25751 -0.49652 0.16322 -0.22434 -0.31205 0.055092 -0.22147 0.29359 -0.12412 -0.28137 0.27464 -0.069968 -0.15021 -0.22195 0.091413 -0.10794 0.32411 0.10797 0.24409 0.19349 -0.052584 -0.58191 -0.34929 -0.4247 0.56021 0.066951 0.44487 -0.017402 0.33296 0.16782 -0.60781 0.058356 0.044058 0.15465 0.18498 0.2521 -0.62418 -0.039028 -0.18849 0.12551 0.32745 -0.14099 0.31894 -0.095142 -0.07607 -0.17728 0.37005 -0.11972 0.32802 -0.074114 0.15685 -0.22622 -0.36427 0.23268 0.66983 0.4231 0.39486 -0.52441 -0.0040785 0.05352 -0.36026 -0.50696 -0.50728 0.10685 0.20806 -0.28306 -0.25915 -0.011765 -0.18617 0.19399 0.20752 0.14626 0.30932 -0.064216 0.25371 0.019049 0.43659 0.44512 0.070055 -0.22787 -0.15639 0.35225 -0.097078 0.37719 -0.023959 -0.2724 -0.2691 -0.11842 0.12733 -0.25782 0.079936 0.28886 -0.26477 0.51567 0.037181 -0.015981 -0.049755 -0.39768 0.0038855\r\n",
      "chanty -0.15577 -0.049188 -0.064377 0.2236 -0.20146 -0.038963 0.12971 -0.29451 0.0035897 -0.098377 -0.30939 0.050878 0.24574 -0.25382 -0.048145 0.15506 -0.39446 0.086549 0.22096 0.010005 -0.029974 -0.16488 -0.51622 -0.00016491 -0.11421 0.26008 -0.19419 0.36296 0.0099712 0.1731 -0.1477 -0.78212 0.19243 -0.14533 0.41308 0.0048941 -0.33375 -0.20914 0.26039 0.10949 0.49339 0.089623 -0.020955 0.15683 0.3137 -0.11759 -0.31317 0.69917 -0.13166 0.64363 -0.23016 0.20046 0.14912 -0.075741 -0.0029152 0.52797 0.18252 0.091756 -0.34982 -0.082495 0.0635 -0.30766 -0.13771 -0.55364 -3.6811e-05 0.11055 -0.42719 -0.21429 0.12669 -0.08624 0.23952 -0.037189 0.273 0.26172 -0.44486 0.10141 0.23421 0.0083713 0.71138 0.31115 -0.39297 -0.26296 0.40601 0.20615 0.20524 -0.11601 0.0101 0.15099 0.13692 0.18864 0.093324 0.094486 -0.023469 -0.48099 0.62332 0.024318 -0.27587 0.075044 -0.5638 0.14501\r\n",
      "kronik -0.094426 0.14725 -0.15739 0.071966 -0.29845 0.039432 0.02187 0.0080409 -0.18682 -0.31101 0.043422 0.16147 -0.012647 0.050696 -0.050954 0.013533 -0.20035 0.3019 -0.010799 -0.19664 -0.26712 -0.38311 -0.08666 -0.10954 0.0042728 -0.15433 0.15416 0.22333 0.13355 0.076866 0.045246 -0.00021332 0.0067774 0.047134 0.32453 -0.31853 -0.35445 -0.21979 -0.12723 0.10492 0.26715 0.058452 0.16751 0.31884 0.046914 -0.16315 -0.078414 0.50551 0.32689 0.046858 0.041268 0.49351 -0.44075 -0.15669 0.62512 0.59474 0.084773 0.017492 -0.89279 -0.28656 0.39685 -0.35591 -0.15007 -0.12261 -0.2569 0.08311 0.013643 0.16162 -0.006617 0.015909 -0.16797 0.11139 0.09692 -0.006589 -0.26508 -0.11282 0.034702 -0.022573 0.44549 0.30833 -0.20067 -0.033317 0.10966 -0.18257 0.54201 -0.11415 -0.19819 0.28277 -0.34232 0.3163 -0.30545 -0.011082 0.11855 -0.11312 0.33951 -0.22449 0.25743 0.63143 -0.2009 -0.10542\r\n",
      "rolonda 0.36088 -0.16919 -0.32704 0.098332 -0.4297 -0.18874 0.45556 0.28529 0.3034 -0.36683 -0.13923 0.10053 -0.52026 -0.30629 -0.18236 0.23908 -0.45987 0.35561 0.067856 0.069954 -0.044425 -0.19452 -0.30248 -0.31011 0.43554 0.24623 0.05153 0.31476 0.094553 0.32482 0.38447 -0.099659 0.414 0.25902 0.08238 0.096832 0.22163 -0.47655 0.13628 0.12927 0.26019 0.45182 -0.079522 0.72982 -0.56586 -0.18653 -0.6082 0.16635 -0.52492 0.14167 -0.26089 -0.046457 -0.060964 -0.48269 0.32158 0.7501 0.52608 0.2913 -0.46036 -0.39314 0.17445 -0.25116 -0.2416 -0.33391 0.086837 0.1027 0.074325 -0.29071 0.3768 0.0079988 0.42131 -0.30349 0.11643 0.38284 0.030575 0.11889 0.42949 -0.0054543 0.83973 0.16628 0.087226 -0.2906 0.16843 -0.19309 0.35477 0.24789 0.14577 0.31387 -0.084938 0.21647 -0.044082 0.14003 0.30007 -0.12731 -0.14304 -0.069396 0.2816 0.27139 -0.29188 0.16109\r\n",
      "zsombor -0.10461 -0.5047 -0.49331 0.13516 -0.36371 -0.4475 0.18429 -0.05651 0.40474 -0.72583 0.31079 -0.31763 -0.019824 -0.29765 0.16847 -0.029003 -0.42048 0.039778 0.10003 0.14749 -0.24683 0.040093 -0.0938 0.32488 -0.22667 -0.039094 0.21616 0.4959 0.069714 0.16686 -0.026112 0.096436 0.18843 -0.3967 0.082282 -0.38073 0.086211 -0.40775 -0.1726 -0.29836 0.29015 -0.0774 0.017294 0.32361 -0.22261 -0.72733 -0.070333 0.17454 0.021926 0.37076 0.37268 0.037672 -0.29863 -0.9022 0.28775 0.60194 0.028256 -0.15408 -0.39262 -0.22826 0.10673 -0.3631 0.35778 0.034102 -0.29885 0.42406 -0.57664 -0.40484 -0.15435 0.23217 -0.014499 0.2932 -0.030599 0.62079 -0.02442 -0.22534 0.13813 -0.21491 0.61883 0.047665 -0.2661 -0.35747 0.32165 -0.53815 0.63114 0.10025 0.22458 0.28004 -0.048782 0.72537 0.15153 -0.10842 0.34064 -0.40916 -0.081263 0.095315 0.15018 0.42527 -0.5125 -0.17054\r\n",
      "sandberger 0.28365 -0.6263 -0.44351 0.2177 -0.087421 -0.17062 0.29266 -0.024899 0.26414 -0.17023 0.25817 0.097484 -0.33103 -0.43859 0.0095799 0.095624 -0.17777 0.38886 0.27151 0.14742 -0.43973 -0.26588 -0.024271 0.27186 -0.36761 -0.24827 -0.20815 0.22128 -0.044409 0.021373 0.24594 0.26143 0.29303 0.13281 0.082232 -0.12869 0.1622 -0.22567 -0.060348 0.28703 0.11381 0.34839 0.3419 0.36996 -0.13592 0.0062694 0.080317 0.0036251 0.43093 0.01882 0.31008 0.16722 0.074112 -0.37745 0.47363 0.41284 0.24471 0.075965 -0.51725 -0.49481 0.526 -0.074645 0.41434 -0.1956 -0.16544 -0.045649 -0.40153 -0.13136 -0.4672 0.18825 0.2612 0.16854 0.22615 0.62992 -0.1288 0.055841 0.01928 0.024572 0.46875 0.2582 -0.31672 0.048591 0.3277 -0.50141 0.30855 0.11997 -0.25768 -0.039867 -0.059672 0.5525 0.13885 -0.22862 0.071792 -0.43208 0.5398 -0.085806 0.032651 0.43678 -0.82607 -0.15701\r\n"
     ]
    }
   ],
   "source": [
    "!tail {os.path.join(glove_dir,'glove.6B.100d.txt')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build embeddings index with word as key and embedding index as value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir,'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:],dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build an embedding matrix that will be load into an Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index['man'].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "\n",
    "# initialize (10000, 100) matrix with zeros \n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "\n",
    "# loop through word in index \n",
    "# (all unique words from document in numeric value)\n",
    "for word, i in word_index.items():\n",
    "    # find word in index and index as value\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if i < max_words:\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106398"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(word_index.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
